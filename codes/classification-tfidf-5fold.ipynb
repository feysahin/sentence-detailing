{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:14.159283Z",
     "iopub.status.busy": "2022-12-20T17:36:14.158469Z",
     "iopub.status.idle": "2022-12-20T17:36:22.911421Z",
     "shell.execute_reply": "2022-12-20T17:36:22.910091Z"
    },
    "papermill": {
     "duration": 8.763501,
     "end_time": "2022-12-20T17:36:22.914349",
     "exception": false,
     "start_time": "2022-12-20T17:36:14.150848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "import itertools\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "import gensim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from trtokenizer.tr_tokenizer import SentenceTokenizer, WordTokenizer\n",
    "word_tokenizer_object = WordTokenizer()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = str(text).lower()   \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "    words = word_tokenizer_object.tokenize(text)\n",
    "    stopWords = set(stopwords.words('turkish'))\n",
    "    new_text = ' '.join(word for word in words if word not in stopWords)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:22.925821Z",
     "iopub.status.busy": "2022-12-20T17:36:22.925121Z",
     "iopub.status.idle": "2022-12-20T17:36:24.363268Z",
     "shell.execute_reply": "2022-12-20T17:36:24.362300Z"
    },
    "papermill": {
     "duration": 1.446753,
     "end_time": "2022-12-20T17:36:24.366128",
     "exception": false,
     "start_time": "2022-12-20T17:36:22.919375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_10_topic.csv', sep='\\t')\n",
    "df_1000 = pd.read_csv('df_1000.csv', sep='\\t')\n",
    "df_hel = pd.read_csv('df_1000_helsinki.csv', sep='\\t')\n",
    "df_det = pd.read_csv('df_1000_detailed.csv', sep='\\t')\n",
    "df_2000 = pd.concat([df_1000, df_1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000['title'][908]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hel['title'][1908]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_det['title'][1908]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:24.411180Z",
     "iopub.status.busy": "2022-12-20T17:36:24.410740Z",
     "iopub.status.idle": "2022-12-20T17:36:24.419226Z",
     "shell.execute_reply": "2022-12-20T17:36:24.417963Z"
    },
    "papermill": {
     "duration": 0.017325,
     "end_time": "2022-12-20T17:36:24.421919",
     "exception": false,
     "start_time": "2022-12-20T17:36:24.404594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = np.unique(df_1000['topic'])\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hel[df_hel['title'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hel = df_hel.drop(index=[264, 314, 341, 359, 1264, 1314, 1341, 1359])\n",
    "df_det = df_det.drop(index=[264, 314, 341, 359, 1264, 1314, 1341, 1359])\n",
    "df_2000 = df_2000.drop(index=[264, 314, 341, 359])\n",
    "df_1000 = df_1000.drop(index=[264, 314, 341, 359])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:24.434182Z",
     "iopub.status.busy": "2022-12-20T17:36:24.433328Z",
     "iopub.status.idle": "2022-12-20T17:36:24.677752Z",
     "shell.execute_reply": "2022-12-20T17:36:24.676430Z"
    },
    "papermill": {
     "duration": 0.253344,
     "end_time": "2022-12-20T17:36:24.680404",
     "exception": false,
     "start_time": "2022-12-20T17:36:24.427060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 4))\n",
    "sns.countplot(df_1000['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276806</td>\n",
       "      <td>sakarya’nın serdivan ilçesinde serdivanspor ka...</td>\n",
       "      <td>sahada başlayan kavga tribüne sıçradı 8 kırmız...</td>\n",
       "      <td>Spor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276807</td>\n",
       "      <td>avustralya başbakanı scott morrison merkezde k...</td>\n",
       "      <td>nauru gözaltı merkezinde sığınmacı çocuk kalma...</td>\n",
       "      <td>Dünya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276808</td>\n",
       "      <td>seat soğuk kış günlerinde otomobil camlarındak...</td>\n",
       "      <td>seat’tan kış ayları 4 öneri</td>\n",
       "      <td>Otomobil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276811</td>\n",
       "      <td>i lerleyen yaşla birlikte cildin elastikiyetin...</td>\n",
       "      <td>kış güneşi cildinizi hızla yaşlandırıyor</td>\n",
       "      <td>Sağlık</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276813</td>\n",
       "      <td>sudan dışişleri bakanı ahmed oac hükümeti sila...</td>\n",
       "      <td>oacde nihai barış anlaşması çarşamba imzalanacak</td>\n",
       "      <td>Dünya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0      276806  sakarya’nın serdivan ilçesinde serdivanspor ka...   \n",
       "1      276807  avustralya başbakanı scott morrison merkezde k...   \n",
       "2      276808  seat soğuk kış günlerinde otomobil camlarındak...   \n",
       "3      276811  i lerleyen yaşla birlikte cildin elastikiyetin...   \n",
       "4      276813  sudan dışişleri bakanı ahmed oac hükümeti sila...   \n",
       "\n",
       "                                               title     topic  \n",
       "0  sahada başlayan kavga tribüne sıçradı 8 kırmız...      Spor  \n",
       "1  nauru gözaltı merkezinde sığınmacı çocuk kalma...     Dünya  \n",
       "2                        seat’tan kış ayları 4 öneri  Otomobil  \n",
       "3           kış güneşi cildinizi hızla yaşlandırıyor    Sağlık  \n",
       "4   oacde nihai barış anlaşması çarşamba imzalanacak     Dünya  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.title = [clean_text(text) for text in test.title]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for n_fold, (_, val_index) in enumerate(kf.split(df_2000[:len(df_2000)//2])):\n",
    "    after_1000 = np.asarray([(i + 1000) for i in val_index])\n",
    "    val_index = np.concatenate((val_index, after_1000))\n",
    "    train_index = list(set(list(range(0, 2000))) - set(val_index))\n",
    "    print(val_index, train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(model):\n",
    "    \n",
    "    scores_f1 = []\n",
    "    \n",
    "    pipe = make_pipeline(TfidfVectorizer(), StandardScaler(with_mean=False), model)\n",
    "    kf = KFold(n_splits=5, random_state=42, shuffle=True)      \n",
    "    length = len(df)//2\n",
    "\n",
    "    for n_fold, (_, val_i) in enumerate(kf.split(range(length))):\n",
    "        \n",
    "        after_half = np.asarray([(i + length) for i in val_i])\n",
    "        val_index = np.concatenate((val_i, after_half))\n",
    "        train_index = list(set(list(range(0, len(df)))) - set(val_index))\n",
    "\n",
    "        X_train = df.iloc[train_index]['title']\n",
    "        y_train = df.iloc[train_index]['topic']\n",
    "\n",
    "        #X_valid = df.iloc[val_index]['title']\n",
    "        #y_valid = df.iloc[val_index]['topic']\n",
    "        \n",
    "        X_valid = test['title']\n",
    "        y_valid = test['topic']\n",
    "\n",
    "        m = pipe.fit(X_train, y_train)\n",
    "        y_pred = m.predict(X_valid)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred, average='macro')        \n",
    "        scores_f1.append(f1)\n",
    "        \n",
    "    print(np.mean(scores_f1), '\\t', np.std(scores_f1))\n",
    "    \n",
    "    return pipe, np.mean(scores_f1), np.std(scores_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df_1000, df_2000, df_hel, df_det]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_f1s = []\n",
    "lsvc_stds = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    df = copy.deepcopy(dataset)\n",
    "    df.title = [clean_text(text) for text in df.title]\n",
    "    \n",
    "    lsvc_pipe, lsvc_f1_mean, lsvc_std = execute_model(LinearSVC())\n",
    "    lsvc_f1s.append(lsvc_f1_mean)\n",
    "    lsvc_stds.append(lsvc_std)\n",
    "\n",
    "lsvc_f1s =  [f\"{num:.3f}\" for num in lsvc_f1s]\n",
    "lsvc_stds =  [f\"{num:.3f}\" for num in lsvc_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_f1s = []\n",
    "rf_stds = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    df = copy.deepcopy(dataset)    \n",
    "    df.title = [clean_text(text) for text in df.title]\n",
    "    \n",
    "    rf_pipe, rf_f1_mean, rf_std = execute_model(RandomForestClassifier())\n",
    "    rf_f1s.append(rf_f1_mean)\n",
    "    rf_stds.append(rf_std)\n",
    "    \n",
    "rf_f1s =  [f\"{num:.3f}\" for num in rf_f1s]\n",
    "rf_stds =  [f\"{num:.3f}\" for num in rf_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_f1s = []\n",
    "lr_stds = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    df = copy.deepcopy(dataset)   \n",
    "    df.title = [clean_text(text) for text in df.title]\n",
    "    \n",
    "    lr_pipe, lr_f1_mean, lr_std = execute_model(LogisticRegression())\n",
    "    lr_f1s.append(lr_f1_mean)\n",
    "    lr_stds.append(lr_std)\n",
    "    \n",
    "lr_f1s =  [f\"{num:.3f}\" for num in lr_f1s]\n",
    "lr_stds =  [f\"{num:.3f}\" for num in lr_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nF1-Score Table:\")\n",
    "f1_df = {'Dataset': ['ds_1000', 'ds_1000_copy', 'ds_1000_back', 'ds_1000_det'], \n",
    "             'Linear_SVC': lsvc_f1s, \n",
    "             'Random_forest': rf_f1s, \n",
    "             'Logistic_regression': lr_f1s}\n",
    "\n",
    "f1_df = pd.DataFrame.from_dict(f1_df).set_index('Dataset')\n",
    "f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStandard Deviation Table:\")\n",
    "std_df = {'Dataset': ['ds_1000', 'ds_1000_copy', 'ds_1000_back', 'ds_1000_det'], \n",
    "             'Linear_SVC': lsvc_stds, \n",
    "             'Random_forest': rf_stds, \n",
    "             'Logistic_regression': lr_stds}\n",
    "\n",
    "std_df = pd.DataFrame.from_dict(std_df).set_index('Dataset')\n",
    "std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 250.10437,
   "end_time": "2022-12-20T17:40:14.329752",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-20T17:36:04.225382",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
