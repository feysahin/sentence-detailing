{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:14.159283Z",
     "iopub.status.busy": "2022-12-20T17:36:14.158469Z",
     "iopub.status.idle": "2022-12-20T17:36:22.911421Z",
     "shell.execute_reply": "2022-12-20T17:36:22.910091Z"
    },
    "papermill": {
     "duration": 8.763501,
     "end_time": "2022-12-20T17:36:22.914349",
     "exception": false,
     "start_time": "2022-12-20T17:36:14.150848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "import itertools\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "import gensim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gensim.models import KeyedVectors\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from trtokenizer.tr_tokenizer import SentenceTokenizer, WordTokenizer\n",
    "word_tokenizer_object = WordTokenizer()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:22.925821Z",
     "iopub.status.busy": "2022-12-20T17:36:22.925121Z",
     "iopub.status.idle": "2022-12-20T17:36:24.363268Z",
     "shell.execute_reply": "2022-12-20T17:36:24.362300Z"
    },
    "papermill": {
     "duration": 1.446753,
     "end_time": "2022-12-20T17:36:24.366128",
     "exception": false,
     "start_time": "2022-12-20T17:36:22.919375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_10_topic.csv', sep='\\t')\n",
    "df_1000 = pd.read_csv('df_1000.csv', sep='\\t')\n",
    "df_hel = pd.read_csv('df_1000_helsinki.csv', sep='\\t')\n",
    "df_det = pd.read_csv('df_1000_detailed.csv', sep='\\t')\n",
    "df_2000 = pd.concat([df_1000, df_1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000['title'][509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hel['title'][1509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_det['title'][1509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:24.411180Z",
     "iopub.status.busy": "2022-12-20T17:36:24.410740Z",
     "iopub.status.idle": "2022-12-20T17:36:24.419226Z",
     "shell.execute_reply": "2022-12-20T17:36:24.417963Z"
    },
    "papermill": {
     "duration": 0.017325,
     "end_time": "2022-12-20T17:36:24.421919",
     "exception": false,
     "start_time": "2022-12-20T17:36:24.404594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = np.unique(df_1000['topic'])\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hel = df_hel.drop(index=[264, 314, 341, 359, 1264, 1314, 1341, 1359])\n",
    "df_det = df_det.drop(index=[264, 314, 341, 359, 1264, 1314, 1341, 1359])\n",
    "df_2000 = df_2000.drop(index=[264, 314, 341, 359])\n",
    "df_1000 = df_1000.drop(index=[264, 314, 341, 359])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T17:36:24.434182Z",
     "iopub.status.busy": "2022-12-20T17:36:24.433328Z",
     "iopub.status.idle": "2022-12-20T17:36:24.677752Z",
     "shell.execute_reply": "2022-12-20T17:36:24.676430Z"
    },
    "papermill": {
     "duration": 0.253344,
     "end_time": "2022-12-20T17:36:24.680404",
     "exception": false,
     "start_time": "2022-12-20T17:36:24.427060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 4))\n",
    "sns.countplot(df_1000['topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(\"C:/Users/asus/NLP/STAR/NEW_DATASET/DATA-AUG/trmodel\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()   \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "    words = word_tokenizer_object.tokenize(text)\n",
    "    stopWords = set(stopwords.words('turkish'))\n",
    "    new_text = [word for word in words if word not in stopWords]\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['kelime'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.title = [clean_text(text) for text in test.title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test['title'][0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "test['topic']= label_encoder.fit_transform(test['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = np.array([np.array([word_vectors[i] for i in ls if i in word_vectors]) for ls in test['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_avg(vect):\n",
    "    vect_avg = []\n",
    "    for v in vect:\n",
    "        if v.size:\n",
    "            vect_avg.append(v.mean(axis=0))\n",
    "        else:\n",
    "            vect_avg.append(np.zeros(400, dtype=float))\n",
    "    return vect_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect_avg = vect_avg(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(model):\n",
    "    \n",
    "    scores_f1 = []\n",
    "    \n",
    "    pipe = make_pipeline(model)\n",
    "    kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    length = len(df)//2\n",
    "\n",
    "    for n_fold, (_, val_i) in enumerate(kf.split(range(length))):\n",
    "        \n",
    "        after_half = np.asarray([(i + length) for i in val_i])\n",
    "        val_index = np.concatenate((val_i, after_half))\n",
    "        train_index = list(set(list(range(0, len(df)))) - set(val_index))\n",
    "        \n",
    "\n",
    "        X_train = df.iloc[train_index]['title']\n",
    "        y_train = df.iloc[train_index]['topic']\n",
    "        \n",
    "        X_valid = test['title']\n",
    "        y_valid = test['topic']\n",
    "        \n",
    "        \n",
    "        X_train_vect = np.array([np.array([word_vectors[i] for i in ls if i in word_vectors]) for ls in X_train])    \n",
    "        X_train_vect_avg = vect_avg(X_train_vect)\n",
    "        \n",
    "        X_test_vect = np.array([np.array([word_vectors[i] for i in ls if i in word_vectors]) for ls in X_valid])\n",
    "        X_test_vect_avg = vect_avg(X_test_vect)\n",
    "        \n",
    "        pipe.fit(X_train_vect_avg, y_train.values.ravel())\n",
    "        y_pred = pipe.predict(X_test_vect_avg)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred, average='macro')        \n",
    "        scores_f1.append(f1)\n",
    "        \n",
    "    print(np.mean(scores_f1), '\\t', np.std(scores_f1))\n",
    "    \n",
    "    return pipe, np.mean(scores_f1), np.std(scores_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df_1000, df_2000, df_hel, df_det]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_f1s = []\n",
    "lsvc_stds = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    df = copy.deepcopy(dataset)\n",
    "    \n",
    "    df.title = [clean_text(text) for text in df.title]\n",
    "    df['topic']= label_encoder.transform(df['topic'])\n",
    "    \"\"\"X_train_vect = np.array([np.array([word_vectors[i] for i in ls if i in word_vectors]) for ls in df['title']])\n",
    "    \n",
    "    X_train_vect_avg = vect_avg(X_train_vect)\"\"\"\n",
    "    \n",
    "    lsvc_pipe, lsvc_f1_mean, lsvc_std = execute_model(LinearSVC())\n",
    "    lsvc_f1s.append(lsvc_f1_mean)\n",
    "    lsvc_stds.append(lsvc_std)\n",
    "    \n",
    "lsvc_f1s =  [f\"{num:.3f}\" for num in lsvc_f1s]\n",
    "lsvc_stds =  [f\"{num:.3f}\" for num in lsvc_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_f1s = []\n",
    "rf_stds = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    df = copy.deepcopy(dataset)\n",
    "    \n",
    "    df.title = [clean_text(text) for text in df.title]\n",
    "    df['topic']= label_encoder.transform(df['topic'])\n",
    "    \n",
    "    rf_pipe, rf_f1_mean, rf_std = execute_model(RandomForestClassifier())\n",
    "    rf_f1s.append(rf_f1_mean)\n",
    "    rf_stds.append(rf_std)\n",
    "    \n",
    "rf_f1s =  [f\"{num:.3f}\" for num in rf_f1s]\n",
    "rf_stds =  [f\"{num:.3f}\" for num in rf_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_f1s = []\n",
    "lr_stds = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    df = copy.deepcopy(dataset)\n",
    "    \n",
    "    df.title = [clean_text(text) for text in df.title]\n",
    "    df['topic']= label_encoder.transform(df['topic'])\n",
    "    \n",
    "    lr_pipe, lr_f1_mean, lr_std = execute_model(LogisticRegression())\n",
    "    lr_f1s.append(lr_f1_mean)\n",
    "    lr_stds.append(lr_std)\n",
    "    \n",
    "lr_f1s =  [f\"{num:.3f}\" for num in lr_f1s]\n",
    "lr_stds =  [f\"{num:.3f}\" for num in lr_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nF1-Score Table:\")\n",
    "f1_df = {'Dataset': ['ds_1000', 'ds_1000_copy', 'ds_1000_back', 'ds_1000_det'], \n",
    "             'Linear_SVC': lsvc_f1s, \n",
    "             'Random_forest': rf_f1s, \n",
    "             'Logistic_regression': lr_f1s}\n",
    "\n",
    "f1_df = pd.DataFrame.from_dict(f1_df).set_index('Dataset')\n",
    "f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6434ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStandard Deviation Table:\")\n",
    "std_df = {'Dataset': ['ds_1000', 'ds_1000_copy', 'ds_1000_back', 'ds_1000_det'], \n",
    "             'Linear_SVC': lsvc_stds, \n",
    "             'Random_forest': rf_stds, \n",
    "             'Logistic_regression': lr_stds}\n",
    "\n",
    "std_df = pd.DataFrame.from_dict(std_df).set_index('Dataset')\n",
    "std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 250.10437,
   "end_time": "2022-12-20T17:40:14.329752",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-20T17:36:04.225382",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
